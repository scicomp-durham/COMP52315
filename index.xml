<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Introduction on COMP52315 – Performance Engineering, Vectorisation and GPU Programming</title><link>https://scicomp-durham.github.io/COMP52315/</link><description>Recent content in Introduction on COMP52315 – Performance Engineering, Vectorisation and GPU Programming</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://scicomp-durham.github.io/COMP52315/index.xml" rel="self" type="application/rss+xml"/><item><title>Benchmarking with `likwid-bench`</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise01/</guid><description>Benchmarking with likwid-bench # We&amp;rsquo;re going to look at the throughput of a very simple piece of code
float reduce(int N, const float *restrict a) { float c = 0; for (int i = 0; i &amp;lt; N; i++) c += a[i]; return c; } when the data live in L1 cache.
We&amp;rsquo;ll do so on an AVX-capable core (where the single-precision vector width is 8 bytes).
There is a loop-carried dependency on the summation variable, and without unrolling the execution stalls at every add until the previous one completes.</description></item><item><title>Introduction</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/introduction/</guid><description>Resources in stored program computers # To understand the performance of a code, we need to have an understanding of what hardware resources it uses, and what resources the hardware provides.
All modern general purpose hardware uses a von Neumann architecture. That is, there is a memory which stores both the program code to be executed and the data for the program. This is attached to a processor (CPU) which contains execution units for executing individual instructions in the program code along with further parts of logical control and load/store of data.</description></item><item><title>Resources in stored program computers</title><link>https://scicomp-durham.github.io/COMP52315/notes/introduction/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/notes/introduction/</guid><description>Resources in stored program computers # To understand the performance of a code, we need to have an understanding of what hardware resources it uses, and what resources the hardware provides.
All modern general purpose hardware uses a von Neumann architecture. That is, there is a memory which stores both the program code to be executed and the data for the program. This is attached to a processor (CPU) which contains execution units for executing individual instructions in the program code along with further parts of logical control and load/store of data.</description></item><item><title>Slides</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/slides/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/slides/</guid><description>Slides 2021/22 edition # Slides for the live lectures. These will be augmented with annotated versions, links to recordings, and some short commentary after the fact. If you think you should have access to the recordings but don&amp;rsquo;t, please get in touch.
The long form notes add words in between the bullet points.
Session 1, annotated, video.
We introduced some ideas of computer architecture and talked about the motivation for the course.</description></item><item><title>Sum reductions</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise01/</guid><description>Benchmarking with likwid-bench # Overview # We&amp;rsquo;re going to look at the throughput of a very simple piece of code
float reduce(int N, const double *restrict a) { float c = 0; for (int i = 0; i &amp;lt; N; i++) c += a[i]; return c; } when all of the data live in L1 cache.
We&amp;rsquo;ll do so on an AVX-capable core (where the single-precision vector width is 8).</description></item><item><title>Caches</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise02/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise02/</guid><description>Measuring memory bandwidth in the memory hierarchy # The goal is to determine the memory bandwidth as a function of how much data we are moving on the Hamilton cores.
Again, as for the first exercise we will do this with likwid-bench. This time, however, we will use three different benchmarks
clcopy: Double-precision cache line copy, only touches first element of each cache line. clload: Double-precision cache line load, only loads first element of each cache line.</description></item><item><title>Journal</title><link>https://scicomp-durham.github.io/COMP52315/journal/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/journal/</guid><description>Journal # Slides that will be used in the lectures. A link to the Panopto recording, if available, as well as a short summary of what was discussed, will follow after the session. If you think you should have access to the recordings but you don&amp;rsquo;t, please get in touch.
Session 1: Slides – Notes – Exercise 1 – Audio
The Encore capture system had some troubles, thus only the audio recording of the session is available.</description></item><item><title>Memory bandwidth in the memory hierarchy</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise02/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise02/</guid><description>Memory bandwidth in the memory hierarchy # The goal of this exercise is to determine the memory bandwidth as a function of the amount of data we are moving on the Hamilton cores.
As done in the the first exercise we will use likwid-bench. This time we will use three different benchmarks:
clcopy: Double-precision cache line copy, which only touches first element of each cache line.
clload: Double-precision cache line load, which only loads first element of each cache line.</description></item><item><title>Overview of memory hierarchies</title><link>https://scicomp-durham.github.io/COMP52315/notes/memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/notes/memory/</guid><description>Overview of memory hierarchies # Reduction benchmark # In exercise 1 you looked at the performance of a vectorised and non-vectorised version of a very simple loop computing the sum of an array of floating point numbers.
In doing so, you produced a plot of the performance (in terms of floating point throughput) as a function of array size. You should have observed something similar to that shown here.</description></item><item><title>The memory hierarchy</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/memory/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/memory/</guid><description>An overview of memory hierarchies # Reduction benchmark # In exercise 1 you looked at the performance of a vectorised and non-vectorised version of a very simple loop computing the sum of an array of floating point numbers.
In doing so, you produced a plot of the performance (in terms of floating point throughput) as a function of array size. You should have observed something similar to that shown here.</description></item><item><title>Memory bandwidth</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise03/</guid><description>Measuring multi-core memory bandwidth # The goal of this exercise is to measure the memory bandwidth for various vector sizes as a function of the number of cores used to process the vector.
Again, we will do this with likwid-bench. This time, we will use the clload benchmark.
Setup # As usual, you should do this on a compute node on Hamilton. See the quickstart in exercise 1 if you can&amp;rsquo;t remember how to do this.</description></item><item><title>Multi-core memory bandwidth</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise03/</guid><description>Multi-core memory bandwidth # The goal of this exercise is to measure the memory bandwidth for various vector sizes as a function of the number of cores used to process the vector.
Again, we will do this with likwid-bench. This time, we will use the clload benchmark.
Topology of a compute node # Exercise The first thing we need to do is figure out what the topology of the node we&amp;rsquo;re running on is.</description></item><item><title>Performance models: roofline</title><link>https://scicomp-durham.github.io/COMP52315/notes/roofline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/notes/roofline/</guid><description>Performance models: roofline # If our goal is to improve the performance of some code we should take a scientific approach. We must first define what we mean by performance. So far, we&amp;rsquo;ve talked about floating point throughput (GFlops/s) or memory bandwidth (GBytes/s). However, these are really secondary characteristics to the primary metric of performance of a code:
How long do I have to wait until I get the answer?</description></item><item><title>Performance models: roofline</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/roofline/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/roofline/</guid><description>Models of performance # If our goal is to improve the performance of some code we should take a scientific approach. We must first define what we mean by performance. So far, we&amp;rsquo;ve talked about floating point throughput (GFlops/s) or memory bandwidth (GBytes/s). However, these are really secondary characteristics to the primary metric of performance of a code:
How long do I have to wait until I get the answer?</description></item><item><title>Measurement and profiling</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/measurements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/measurements/</guid><description>Performance measurements # So far, we&amp;rsquo;ve seen the roofline model, and observed that for floating point code, it allows us to get a high-level view of what coarse step we should be taking to improve the performance of our algorithm.
Now suppose that we do a roofline analysis for our code, we observe that it should be limited by floating point throughput, but we are nowhere near the roof.</description></item><item><title>Performance measurements and profiling</title><link>https://scicomp-durham.github.io/COMP52315/notes/measurements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/notes/measurements/</guid><description>Performance measurements and profiling # So far, we&amp;rsquo;ve seen the roofline model, and observed that for floating point code, it allows us to get a high-level view of what coarse step we should be taking to improve the performance of our algorithm.
Now suppose that we do a roofline analysis for our code, we observe that it should be limited by floating point throughput, but we are nowhere near the roof.</description></item><item><title>Roofline analysis</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise04/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise04/</guid><description>A roofline analysis of matrix-vector multiplication # The goal of this exercise is to perform a roofline analysis of matrix-vector multiplication. We will look at the effect that compiler flags have on the performance of generated code. Next, we will see if the performance that we obtain is independent of the problem size. Finally, we will investigate loop blocking.
Background # I provide an implementation1 in code/exercise04/dmvm.c, in C, of double-precision matrix-vector multiplication, which computes</description></item><item><title>Roofline analysis of matrix–vector multiplication</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise04/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise04/</guid><description>Roofline analysis of matrix–vector multiplication # The goal of this exercise is to perform a roofline analysis of matrix–vector multiplication. We will look at the effect that compiler flags have on the performance of the generated code. Next, we will check whether the performance we obtain depends on the problem size. Finally, we will investigate loop blocking.
Background # In code/exercise04/dmvm.c, you can find a C implementation1of matrix–vector multiplication, which computes</description></item><item><title>Cache blocking/tiling</title><link>https://scicomp-durham.github.io/COMP52315/notes/tiling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/notes/tiling/</guid><description>Achieving reasonable performance for loopy code # Many of the algorithms we encounter in scientific computing have quite &amp;ldquo;simple&amp;rdquo; data access patterns. Numerical code often has multiple nested loops with regular array indexing. This is actually a reason the roofline model is so successful: its optimistic assumptions are not too optimistic.
Despite this simplicity, on hardware with memory caches, we still need to do some work to turn this &amp;ldquo;simple&amp;rdquo; code into something that runs with reasonable performance.</description></item><item><title>Cache blocking/tiling</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/tiling/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/notes/tiling/</guid><description>Achieving reasonable performance for loopy code # Many of the algorithms we encounter in scientific computing have quite &amp;ldquo;simple&amp;rdquo; data access patterns. Numerical code often has multiple nested loops with regular array indexing. This is actually a reason the roofline model is so successful: its optimistic assumptions are not too optimistic.
Despite this simplicity, on hardware with memory caches, we still need to do some work to turn this &amp;ldquo;simple&amp;rdquo; code into something that runs with reasonable performance.</description></item><item><title>Coursework: fast finite elements</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/coursework/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/coursework/</guid><description>Fast finite elements # The submission deadline for this work is 2022-02-21 at 14:00UTC. Introduction # The finite element method is a popular and flexible method for the numerical solution of partial differential equations. In this coursework, you will study and benchmark some performance optimisations for a prototypical finite element problem.
In particular, we will study the solution of the simplest possible finite element problem, to solve the linear system</description></item><item><title>Further resources</title><link>https://scicomp-durham.github.io/COMP52315/resources/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/resources/</guid><description>Other reading/resources # Travis Downs writes an interesting, and tremendously detailed, low-level performance optimisation and measurement blog. John Regehr writes about compilers often including details of optimisation. For an interesting look at how far you can go down the rabbithole of optimising a small piece of code, see this sequence of posts on improving the performance of sorting small arrays by vectorisation (among other things). Denis Bakhvalov writes a blog on performance optimisation, with a focus on the &amp;ldquo;top-down&amp;rdquo; methodology.</description></item><item><title>Models and measurements</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise05/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise05/</guid><description>Verifying a model with measurements # The goal of this exercise is to verify our model for the number of loads and stores in a stream benchmark using performance counters, accessed via likwid-perfctr.
Background # I provide an implementation (in code/exercise05/stream.c) written in C of the STREAM TRIAD benchmark. It provides scalar, SSE, and AVX implementations of the loop
double *a, *b, *c; ... for (i = 0; i &amp;amp;lt; N; i++) { c[i] = c[i] + a[i] * b[i]; } We will measure the number of loads and stores for this loop using likwid-perfctr.</description></item><item><title>Verifying a model with measurements</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise05/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise05/</guid><description>Verifying a model with measurements # The goal of this exercise is to verify our model for the number of loads and stores in a stream benchmark using performance counters, accessed via likwid-perfctr.
Background # In code/exercise05/stream.c, you can find a C implementation of the STREAM TRIAD benchmark. The code provides a scalar, an SSE, and AVX implementations of the loop
double *a, *b, *c; ... for (i = 0; i &amp;amp;lt; N; i++) { c[i] = c[i] + a[i] * b[i]; } Once compiled, the program can be called as .</description></item><item><title>Finding a hotspot and determining the execution limits</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise06/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise06/</guid><description>Finding a hotspot and determining the execution limits # The goal of this exercise is to model the performance of existing code using the tools we have seen so far: the GNU profiler to profile our code and likwid-perfctr to look at performance counters. We will have to instrument the code using the Marker API macros. This is a rather hand-on exercise, in which you will get to modify existing code before running it.</description></item><item><title>Profiling</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise06/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise06/</guid><description>Finding a hotspot and determining the execution limits # So far, we&amp;rsquo;ve only run very simple benchmarks. Now we&amp;rsquo;re going to try and find some information in a larger code. We will look at the miniMD application which has been developed as part of the Mantevo project. This is a molecular dynamics code that implements algorithms and data structures from a large research code, but in a small package that is amenable to benchmarking and trying out different optimisations.</description></item><item><title>Loop tiling matrix transpose</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise07/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise07/</guid><description>The effect of loop tiling on matrix transpose # In lectures, we saw a model for throughput of a matrix transpose operation. Here we&amp;rsquo;re going to look at the effect on throughput of loop tiling. I provide an implementation of matrix transpose with and without one level of loop tiling.
As usual, these live in the repository. Compile the code # We&amp;rsquo;ll use the intel compiler to build this code.</description></item><item><title>The effect of loop tiling on matrix transposition</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise07/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise07/</guid><description>The effect of loop tiling on matrix transposition # In lectures, we saw a model for throughput of a matrix transpose operation. Here we&amp;rsquo;re going to look at the effect on throughput of loop tiling. You can find an implementation of naive transposition here and one with one level of loop tiling here.
Compile the code # We&amp;rsquo;ll use the Intel compiler to build this code. So after logging in to Hamilton and downloading, load the relevant module</description></item><item><title>Loop tiling for matrix–matrix multiplication</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise08/</guid><description>Loop tiling for matrix–matrix multiplication # In Exercise 7, we looked at the effect of loop tiling schemes to increase the throughput of matrix transposition. Now we are going to look at the throughput of loop tiling for matrix–matrix multiplication. The matrix-matrix multiplication code we will use provides contains different variants: a naive triple loop, a tiled version of the triple loop, and a tiled version that manually packs local buffers.</description></item><item><title>Loop tiling matrix-matrix multiplication</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise08/</guid><description>Simple loop tiling for matrix-matrix multiplication # Having looked at the effect of loop tiling schemes for increasing the throughput of matrix transpose operations in exercise 7, we&amp;rsquo;re now going to look at throughput of the loop-tiling scheme presented in lectures for matrix-matrix multiplication. I provide an implementation of matrix-matrix multiplication in code/exercise08/gemm.c that provides three different variants. A naive triple loop, a tiled version of the triple loop, and a tiled version that manually packs local buffers.</description></item><item><title>Compiler feedback and the BLIS DGEMM</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise09/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise09/</guid><description>Compiler feedback and the BLIS DGEMM # We&amp;rsquo;re going to look at how to interact with the compiler to obtain the so that it vectorise a loop the way we want it to.
As our starting point we will use a C version of the GEMM micro-kernel used in the BLIS framework.
We will only look at the optimization report (and at the assembly code) produced by the compiler, but we will not run the produced binary.</description></item><item><title>Compiler feedback and the BLIS DGEMM</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise09/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise09/</guid><description>Getting compilers to the right thing # We&amp;rsquo;re going to look at how to convince the compiler to vectorise a loop the way we want it to.
As our starting point we&amp;rsquo;ll use a C version of the GEMM micro-kernel used in the BLIS framework.
Rather than doing this on Hamilton, since we&amp;rsquo;re not actually going to run the code, we will use the Compiler explorer which is an online frontend to trying out lots of different compilers.</description></item><item><title>Acknowledgements</title><link>https://scicomp-durham.github.io/COMP52315/acknowledgements/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/acknowledgements/</guid><description>Acknowledgements # The course was developed by Lawrence Mitchell.
Some of the exercises adapt material from the HPC group at FAU. Their likwid tool is invaluable.</description></item><item><title>Stencil layer conditions</title><link>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise10/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/past-editions/2021-22/exercises/exercise10/</guid><description>Loop tiling for stencil codes # We&amp;rsquo;re going to investigate the layer condition loop tiling guidance for stencil codes. We&amp;rsquo;ll use as an exemplar the five-point finite difference stencil for the Laplacian on a regular grid in code/exercise10/fivepoint.c.
The layer condition model was developed by a group at Erlangen, and they provide an interactive calculator.
As usual, we&amp;rsquo;re going to use the Intel compiler on Hamilton, so after logging in and downloading the code, load the relevant modules</description></item><item><title>Contact details</title><link>https://scicomp-durham.github.io/COMP52315/setup/contact/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/setup/contact/</guid><description>Contact details # I am Massimiliano Fasi and I am teaching the course in January 2023. You can contact me by sending an email to massimiliano.fasi@durham.ac.uk.
This website and all course materials are hosted on GitHub. If you have a suggestion or want to report a mistake, please use the GitHub issue tracker. Pull requests are also welcome.</description></item><item><title>Hamilton accounts</title><link>https://scicomp-durham.github.io/COMP52315/setup/hamilton/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/setup/hamilton/</guid><description>Access to Hamilton # For many of the exercises in the course, we will be using the Hamilton supercomputer. If you do not have an account on the machine, please apply for one by completing the registration form on the Self Service Portal. More information on access requests is available here.
We&amp;rsquo;ll be logging in a lot—please follow the tips on how to configure ssh for swifter login.
Hamilton quick start guide # This quick start guide is intended to get you up and running on the Hamilton 8 supercomputer, but it should not be considered a replacement for the official documentation—please refer to it for anything that you cannot find here.</description></item><item><title>ssh configuration</title><link>https://scicomp-durham.github.io/COMP52315/setup/configuration/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/setup/configuration/</guid><description>ssh tips &amp;amp; tricks # Setting up simpler logins # It can be tedious to remember to type long login commands every time when logging in via ssh to Hamilton. I therefore recommend that you set up an ssh config file.
Additionally, you might also want to set up ssh keys for passwordless login.
The ssh-config configuration file # GNU/Linux and MacOS When you run it, ssh reads a configuration file at $HOME/.</description></item><item><title>Unix resources</title><link>https://scicomp-durham.github.io/COMP52315/setup/unix/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/setup/unix/</guid><description>Using Unix-like systems for computational science # This course presupposes some level of familiarity with commandline interfaces. You have encountered these already in the previous term. In case you need a quick refresher, I recommend the material produced by the Software Carpentry project.
They have a number of useful lessons and materials providing introductory training on how to do things like use the Unix shell, version control with git, and some introductory programming and plotting in Python.</description></item></channel></rss>