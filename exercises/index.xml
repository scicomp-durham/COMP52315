<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Exercises on COMP52315 – Performance Engineering, Vectorisation and GPU Programming</title><link>https://scicomp-durham.github.io/COMP52315/exercises/</link><description>Recent content in Exercises on COMP52315 – Performance Engineering, Vectorisation and GPU Programming</description><generator>Hugo -- gohugo.io</generator><atom:link href="https://scicomp-durham.github.io/COMP52315/exercises/index.xml" rel="self" type="application/rss+xml"/><item><title>Benchmarking with `likwid-bench`</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise01/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise01/</guid><description>Benchmarking with likwid-bench # We&amp;rsquo;re going to look at the throughput of a very simple piece of code
float reduce(int N, const float *restrict a) { float c = 0; for (int i = 0; i &amp;lt; N; i++) c += a[i]; return c; } when the data live in L1 cache.
We&amp;rsquo;ll do so on an AVX-capable core (where the single-precision vector width is 8 bytes).
There is a loop-carried dependency on the summation variable, and without unrolling the execution stalls at every add until the previous one completes.</description></item><item><title>Memory bandwidth in the memory hierarchy</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise02/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise02/</guid><description>Memory bandwidth in the memory hierarchy # The goal of this exercise is to determine the memory bandwidth as a function of the amount of data we are moving on the Hamilton cores.
As done in the the first exercise we will use likwid-bench. This time we will use three different benchmarks:
clcopy: Double-precision cache line copy, which only touches first element of each cache line.
clload: Double-precision cache line load, which only loads first element of each cache line.</description></item><item><title>Multi-core memory bandwidth</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise03/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise03/</guid><description>Multi-core memory bandwidth # The goal of this exercise is to measure the memory bandwidth for various vector sizes as a function of the number of cores used to process the vector.
Again, we will do this with likwid-bench. This time, we will use the clload benchmark.
Topology of a compute node # Exercise The first thing we need to do is figure out what the topology of the node we&amp;rsquo;re running on is.</description></item><item><title>Roofline analysis of matrix–vector multiplication</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise04/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise04/</guid><description>Roofline analysis of matrix–vector multiplication # The goal of this exercise is to perform a roofline analysis of matrix–vector multiplication. We will look at the effect that compiler flags have on the performance of the generated code. Next, we will check whether the performance we obtain depends on the problem size. Finally, we will investigate loop blocking.
Background # In code/exercise04/dmvm.c, you can find a C implementation1of matrix–vector multiplication, which computes</description></item><item><title>Verifying a model with measurements</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise05/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise05/</guid><description>Verifying a model with measurements # The goal of this exercise is to verify our model for the number of loads and stores in a stream benchmark using performance counters, accessed via likwid-perfctr.
Background # In code/exercise05/stream.c, you can find a C implementation of the STREAM TRIAD benchmark. The code provides a scalar, an SSE, and AVX implementations of the loop
double *a, *b, *c; ... for (i = 0; i &amp;amp;lt; N; i++) { c[i] = c[i] + a[i] * b[i]; } Once compiled, the program can be called as .</description></item><item><title>Finding a hotspot and determining the execution limits</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise06/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise06/</guid><description>Finding a hotspot and determining the execution limits # The goal of this exercise is to model the performance of existing code using the tools we have seen so far: the GNU profiler to profile our code and likwid-perfctr to look at performance counters. We will have to instrument the code using the Marker API macros. This is a rather hand-on exercise, in which you will get to modify existing code before running it.</description></item><item><title>The effect of loop tiling on matrix transposition</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise07/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise07/</guid><description>The effect of loop tiling on matrix transposition # In lectures, we saw a model for throughput of a matrix transpose operation. Here we&amp;rsquo;re going to look at the effect on throughput of loop tiling. You can find an implementation of naive transposition here and one with one level of loop tiling here.
Compile the code # We&amp;rsquo;ll use the Intel compiler to build this code. So after logging in to Hamilton and downloading, load the relevant module</description></item><item><title>Loop tiling for matrix–matrix multiplication</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise08/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise08/</guid><description>Loop tiling for matrix–matrix multiplication # In Exercise 7, we looked at the effect of loop tiling schemes to increase the throughput of matrix transposition. Now we are going to look at the throughput of loop tiling for matrix–matrix multiplication. The matrix-matrix multiplication code we will use provides contains different variants: a naive triple loop, a tiled version of the triple loop, and a tiled version that manually packs local buffers.</description></item><item><title>Compiler feedback and the BLIS DGEMM</title><link>https://scicomp-durham.github.io/COMP52315/exercises/exercise09/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://scicomp-durham.github.io/COMP52315/exercises/exercise09/</guid><description>Compiler feedback and the BLIS DGEMM # We&amp;rsquo;re going to look at how to interact with the compiler to obtain the so that it vectorise a loop the way we want it to.
As our starting point we will use a C version of the GEMM micro-kernel used in the BLIS framework.
We will only look at the optimization report (and at the assembly code) produced by the compiler, but we will not run the produced binary.</description></item></channel></rss>