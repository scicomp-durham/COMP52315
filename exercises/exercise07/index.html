<!doctype html><html lang=en dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="The effect of loop tiling on matrix transposition #  In lectures, we saw a model for throughput of a matrix transpose operation. Here we&rsquo;re going to look at the effect on throughput of loop tiling. You can find an implementation of naive transposition here and one with one level of loop tiling here.
Compile the code #  We&rsquo;ll use the Intel compiler to build this code. So after logging in to Hamilton and downloading, load the relevant module"><meta name=theme-color content="#FFFFFF"><meta property="og:title" content="The effect of loop tiling on matrix transposition"><meta property="og:description" content="The effect of loop tiling on matrix transposition #  In lectures, we saw a model for throughput of a matrix transpose operation. Here we&rsquo;re going to look at the effect on throughput of loop tiling. You can find an implementation of naive transposition here and one with one level of loop tiling here.
Compile the code #  We&rsquo;ll use the Intel compiler to build this code. So after logging in to Hamilton and downloading, load the relevant module"><meta property="og:type" content="article"><meta property="og:url" content="https://scicomp-durham.github.io/COMP52315/exercises/exercise07/"><meta property="article:modified_time" content="2023-02-08T02:19:00+00:00"><title>The effect of loop tiling on matrix transposition | COMP52315 – Performance Engineering, Vectorisation and GPU Programming</title><link rel=icon href=/COMP52315/favicon.svg type=image/x-icon><link rel=stylesheet href=/COMP52315/book.min.0cb0b7d6a1ed5d0e95321cc15edca4d6e9cc406149d1f4a3f25fd532f6a3bb38.css integrity="sha256-DLC31qHtXQ6VMhzBXtyk1unMQGFJ0fSj8l/VMvajuzg="><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css integrity=sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js integrity=sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js integrity=sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:true},{left:"$",right:"$",display:false},{left:"\\(",right:"\\)",display:false},{left:"\\[",right:"\\]",display:true}]})});</script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><nav><div class=book-brand><img class=book-center src=/COMP52315/logo.svg alt=Logo><h2><a href=/COMP52315>COMP52315 – Performance Engineering, Vectorisation and GPU Programming</a></h2></div><ul><li><span>Course resources</span><ul><li><a href=/COMP52315/setup/contact/>Contact details</a></li><li><a href=/COMP52315/setup/hamilton/>Hamilton accounts</a></li><li><a href=/COMP52315/setup/configuration/>ssh configuration</a></li><li><a href=/COMP52315/setup/unix/>Unix resources</a></li></ul></li><li><a href=/COMP52315/journal/>Journal</a></li><li><a href=/COMP52315/exercises/>Exercises</a><ul></ul></li><li><a href=/COMP52315/notes/>Notes</a><ul></ul></li><li><a href=/COMP52315/resources/>Further resources</a></li><li><a href=/COMP52315/acknowledgements/>Acknowledgements</a></li><li><a href=/COMP52315/past-editions/>Past editions</a><ul><li><a href=/COMP52315/past-editions/2020-21/>Edition 2020/2021</a><ul></ul></li><li><a href=/COMP52315/past-editions/2021-22/>Edition 2021/2022</a><ul></ul></li></ul></li></ul></nav><script>(function(){var menu=document.querySelector("aside.book-menu nav");addEventListener("beforeunload",function(event){localStorage.setItem("menu.scrollTop",menu.scrollTop);});menu.scrollTop=localStorage.getItem("menu.scrollTop");})();</script></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/COMP52315/svg/menu.svg class=book-icon alt=Menu></label>
<strong>The effect of loop tiling on matrix transposition</strong>
<label for=toc-control><img src=/COMP52315/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#the-effect-of-loop-tiling-on-matrix-transposition>The effect of loop tiling on matrix transposition</a><ul><li><a href=#compile-the-code>Compile the code</a></li><li><a href=#measure-effective-bandwidth-as-a-function-of-matrix-size>Measure effective bandwidth as a function of matrix size</a></li><li><a href=#measuring-cache-behaviour>Measuring cache behaviour</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=the-effect-of-loop-tiling-on-matrix-transposition>The effect of loop tiling on matrix transposition
<a class=anchor href=#the-effect-of-loop-tiling-on-matrix-transposition>#</a></h1><p>In lectures, we saw a model for throughput of a matrix transpose
operation. Here we&rsquo;re going to look at the effect on throughput of loop
tiling. You can find an implementation of naive transposition
<a href=https://scicomp-durham.github.io/COMP52315/code/exercise07/transpose.c>here</a> and one with one level of loop
tiling <a href=https://scicomp-durham.github.io/COMP52315/code/exercise07/transpose-blocked.c>here</a>.</p><h2 id=compile-the-code>Compile the code
<a class=anchor href=#compile-the-code>#</a></h2><p>We&rsquo;ll use the Intel compiler to build this code. So after logging in
to Hamilton and downloading, load the relevant module</p><pre><code>module load intel/2022.2
</code></pre><p>The untiled version of the code can be compiled with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>icx -O1 -std<span style=color:#f92672>=</span>c99 -o transpose transpose.c
</code></pre></div><p>and the tiled version with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>icx -O1 -std<span style=color:#f92672>=</span>c99 -o transpose-blocked transpose-blocked.c
</code></pre></div><h2 id=measure-effective-bandwidth-as-a-function-of-matrix-size>Measure effective bandwidth as a function of matrix size
<a class=anchor href=#measure-effective-bandwidth-as-a-function-of-matrix-size>#</a></h2><blockquote class=exercise><h3>Exercise</h3><span>For both the blocked and unblocked code, measure the memory bandwidth
as a function of the number of rows and columns (using square matrices
is fine) from around \(N = 100\) to \(N = 20000\). Try both with \(N\)
a power of two, and \(N\) a multiple of ten.</span></blockquote><blockquote class=question><h3>Question</h3><span>What do you observe comparing the blocked and unblocked performance?</span></blockquote><blockquote class=question><h3>Question</h3><span>Do you notice anything different when using power of two sizes
compared to multiples of ten?</span></blockquote><p>The default blocking size is a \(64 \times 64\) tile. You can
override these sizes when compiling with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>icx -O1 -std<span style=color:#f92672>=</span>c99 -o transpose-blocked transpose-blocked.c -DRSTRIDE<span style=color:#f92672>=</span>X
-DCSTRIDE<span style=color:#f92672>=</span>Y
</code></pre></div><p>by setting <code>X</code> and <code>Y</code> to appropriate numbers.</p><blockquote class=question><h3>Question</h3><span>Given that a Hamilton CPU has a 32kB level one cache size. What is a
good tile size if you want to block for level one cache?</span></blockquote><blockquote class=question><h3>Question</h3><span>Do you notice any performance changes if you change the tile size?</span></blockquote><h2 id=measuring-cache-behaviour>Measuring cache behaviour
<a class=anchor href=#measuring-cache-behaviour>#</a></h2><p>The code is annotated with likwid markers, so we can run it with
<code>likwid-perfctr</code> and measure the cache behaviour. To do this, load the
<code>likwid/5.2.0</code> module and recompile the two executables with</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>icx -O1 -std<span style=color:#f92672>=</span>c99 -DLIKWID_PERFMON -o transpose transpose.c -llikwid
</code></pre></div><p>and</p><div class=highlight><pre style=color:#272822;background-color:#fafafa><code class=language-sh data-lang=sh>icx -O1 -std<span style=color:#f92672>=</span>c99 -DLIKWID_PERFMON -o transpose-blocked transpose-blocked.c -llikwid
</code></pre></div><blockquote class=exercise><h3>Exercise</h3><span>For a \(4096 \times 4096\) matrix, measure the main memory bandwidth
and data volume for both the blocked and unblocked cases with
<code>likwid-perfctr -g MEM -C 0 -m &lt;executable> 4096 4096</code>.</span></blockquote><blockquote class=question><h3>Question</h3><span><p>What do you observe about the measured data volume (reported by
likwid) compared to the effective data volume?</p><p>What about if you change to a \(5000 \times 5000\) matrix?</p><p>Can you explain what you see?</p></span></blockquote></article><footer class=book-footer><div class="flex flex-wrap justify-between"><div><a class="flex align-center" href=https://github.com/scicomp-durham/COMP52315/commit/5e994206d6719c797436471e6da757e835e3610b title="Last modified by Massimiliano Fasi | February 8, 2023" target=_blank rel=noopener><img src=/COMP52315/svg/calendar.svg class=book-icon alt=Calendar>
<span>February 8, 2023</span></a></div><div><a class="flex align-center" href=https://github.com/scicomp-durham/COMP52315/edit/main/site/content//exercises/exercise07.md target=_blank rel=noopener><img src=/COMP52315/svg/edit.svg class=book-icon alt=Edit>
<span>Edit this page</span></a></div></div><div class="flex flex-wrap align-right"><p>© 2020&ndash;2023 <a href=mailto:lawrence@wence.uk>Lawrence Mitchell</a>, <a href=mailto:massimiliano.fasi@durham.ac.uk>Massimiliano Fasi</a>, and <a href=https://www.dur.ac.uk/>Durham University</a>.</p><p><a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/><img alt="Creative Commons License" style=border-width:0 src=/COMP52315/cc-by-sa.svg></a>
This work is licensed under a <a rel=license href=http://creativecommons.org/licenses/by-sa/4.0/>Creative
Commons Attribution-ShareAlike 4.0 International License</a>.</p></div></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><nav id=TableOfContents><ul><li><a href=#the-effect-of-loop-tiling-on-matrix-transposition>The effect of loop tiling on matrix transposition</a><ul><li><a href=#compile-the-code>Compile the code</a></li><li><a href=#measure-effective-bandwidth-as-a-function-of-matrix-size>Measure effective bandwidth as a function of matrix size</a></li><li><a href=#measuring-cache-behaviour>Measuring cache behaviour</a></li></ul></li></ul></nav></aside></main></body></html>